# -*- coding: utf-8 -*-
"""LabAssignment1_yiyanhuang1010.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hPkm8AqkU-ry21L3yb7OlR9EM_qD97yj
"""

# Commented out IPython magic to ensure Python compatibility.
#1(a). generate a dataset from y=5*x**2+3*x+8 with noise
import numpy
num_samples = 1600
x = numpy.random.rand(num_samples, 1)
y = 5*x**2+3*x+8
noise = numpy.random.randn(num_samples, 1)
y_withNoise = y + noise
import matplotlib.pyplot as plt
# %matplotlib inline
plt.plot(x,y,"b.")
plt.plot(x,y_withNoise,"r.")

#1(b). Create an 80/20 train/test split of the dataset
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split (x, y_withNoise, test_size=0.2, random_state = 42)

#1(c1). Train and evaluate the predictive performance for linear regression models 
from sklearn import linear_model
lr_model = linear_model.LinearRegression()
lr_model.fit(x_train, y_train)
y_predicted_lr= lr_model.intercept_ + lr_model.coef_*x_test

from sklearn.metrics import mean_squared_error
evaluation_lr = mean_squared_error(y_test, y_predicted_lr)
print ("The mean squared error for the linear model is:", evaluation_lr)

plt.plot(x_test,y_test,"r.")
plt.plot(x_test,y_predicted_lr,"b.")

#1(c2). Train and evaluate the predictive performance for polynomial (to the 4th degree)
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=4, include_bias=False)
x_polyTrain = poly_features.fit_transform(x_train)
x_polyTest = poly_features.fit_transform(x_test)

#x_polyTrain, x_polyTest, y_polyTrain, y_polyTest = train_test_split (x_poly, y_withNoise, test_size=0.2, random_state=42)

poly_model = linear_model.LinearRegression()
poly_model.fit(x_polyTrain, y_train)
y_predicted_pl = poly_model.predict(x_polyTest)
plt.plot(x_test, y_test, "r.")
plt.plot(x_test, y_predicted_pl, "b.")

from sklearn.metrics import mean_squared_error
evaluation_pl = mean_squared_error(y_test, y_predicted_pl)
print ("The mean squared error for the polynomial is:", evaluation_pl)

#1(c3). Train and evaluate the predictive performance for polynomial (to the 4th degree) with Ridge regularization (regularization strength set to 0.5)

ridge_model = linear_model.Ridge(alpha=0.5)
ridge_model.fit(x_polyTrain, y_polyTrain)
y_predicted_pl_ridge = ridge_model.predict(x_polyTest)
plt.plot(x_test, y_test, "r.")
plt.plot(x_test, y_predicted_pl_ridge, "b.")

from sklearn.metrics import mean_squared_error
evaluation_pl_ridge = mean_squared_error(y_polyTest, y_predicted_pl_ridge)
print ("The mean squared error for the polynomial with Ridge regularization is:", evaluation_pl_ridge)

#1(c4). Train and evaluate the predictive performance for polynomial (to the 4th degree) with Lasso regularization (regularization strength set to 0.5)

lasso_model = linear_model.Lasso(alpha=0.5)
lasso_model.fit(x_polyTrain, y_polyTrain)
y_predicted_pl_lasso = lasso_model.predict(x_polyTest)
plt.plot(x_test, y_test, "r.")
plt.plot(x_test, y_predicted_pl_lasso, "b.")

from sklearn.metrics import mean_squared_error
evaluation_pl_lasso = mean_squared_error(y_polyTest, y_predicted_pl_lasso)
print ("The mean squared error for the polynomial with Lasso regularization is:", evaluation_pl_lasso)

#1(c5).Evaluate using mean squared error. Report all values in a single table. (Code and Write-up)

fig = plt.figure(dpi=120)
ax = fig.add_subplot(1,1,1)
table_data=[
    ["The mean squared error of linear regression", evaluation_lr],
    ["The mean squared error of polynomial regression", evaluation_pl],
    ["The mean squared error of polynomial regression with ridge regularization", evaluation_pl_ridge],
    ["The mean squared error of polynomial regression with lasso regularization", evaluation_pl_lasso],
]
table = ax.table(cellText=table_data, loc='center')
table.set_fontsize(20)
table.scale(2.2,3)
ax.axis('off')

#1(d).Write a discussion analyzing and comparing the two models. Explain which model performs better and why. 
#Also, address which models you think are underfitting versus overfitting and explain why. 
#Your discussion should consist of two to four paragraphs. (Write-up)

"""(a) Load a real dataset not covered in class that is designed for the regression problem;
e.g., from sklearn.datasets, Kaggle, your own data, etc. (Code)
(b) Create a 60/20/20 train/val/test split of the dataset. (Code)
"""

#2(a). Load a real dataset not covered in class that is designed for the regression problem;
from sklearn.datasets import fetch_california_housing
housing_dataset = fetch_california_housing()
print ("Keys of the housing dataset:\n", housing_dataset.keys())
print(housing_dataset["feature_names"])

#2(b) Create a 60/20/20 train/val/test split of the dataset.
from sklearn.model_selection import train_test_split
x_trainandval_c, x_test_c, y_trainandval_c, y_test_c = train_test_split(housing_dataset.data, housing_dataset.target, test_size=0.2, random_state=42)
x_train_c, x_val_c, y_train_c, y_val_c = train_test_split (x_trainandval_c, y_trainandval_c, test_size=0.25, random_state=42)
print("number samples in training: ", len(x_train_c))
print("number samples in testing: ", len(x_test_c))
print("number samples in validation: ", len(x_val_c))

"""(c) Plot performance curves for the polynomial regression model (to the 4th degree)
when varying the regularization strength for each of the following types of regularization: ridge and lasso. Vary the regularization strength to have at least 10 different values and evaluate using mean squared error on the validation split.
Identify the regularization type-value that returns the best result for each regularization type. (Code and Write-up)
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

poly_features_c = PolynomialFeatures(degree=4, include_bias=False)
x_poly_train_c = poly_features_c.fit_transform(x_train_c)
x_poly_val_c = poly_features_c.fit_transform(x_val_c)
x_poly_test_c = poly_features_c.fit_transform(x_test_c)

start_alpha = 1
finish_alpha = 11
increment_amt = 1
alpha_num = 0.33
alpha_value = []
evaluation_alpha = []

for alpha_count in range (start_alpha, finish_alpha, increment_amt):
  ridge_model_c = linear_model.Ridge(alpha= alpha_num)
  ridge_model_c.fit(x_poly_train_c, y_train_c)
  y_predicted_ridge_c = ridge_model_c.predict(x_poly_val_c)
  evaluation_ridge_c = mean_squared_error(y_val_c, y_predicted_ridge_c)
  alpha_value.append(alpha_num)
  evaluation_alpha.append(evaluation_ridge_c)
  alpha_num = alpha_num + 0.01

plt.plot(alpha_value, evaluation_alpha, "b-+", linewidth=2)
plt.xlabel("Alpha values")
plt.ylabel("Mean standard errors")
plt.title("Evaluation metrics of different alpha values for Ridge model")

print (alpha_value, len(alpha_value))
print (evaluation_alpha, len(evaluation_alpha))

"""Best alpha value for Ridge regression is 0.39 with the SE 43117236.74113868"""

start_alpha_l = 1
finish_alpha_l = 11
increment_amt_l = 1
alpha_num_l = 0.01
alpha_value_l = []
evaluation_alpha_l = []
evaluation_alpha_l=[]

for alpha_count in range (start_alpha_l, finish_alpha_l, increment_amt_l):
  lasso_model_c = linear_model.Lasso(alpha= alpha_num_l)
  lasso_model_c.fit(x_poly_train_c, y_train_c)
  y_predicted_lasso_c = lasso_model_c.predict(x_poly_val_c)
  evaluation_lasso_c = mean_squared_error(y_val_c, y_predicted_lasso_c)
  alpha_value_l.append(alpha_num_l)
  evaluation_alpha_l.append(evaluation_lasso_c)
  alpha_num_l = alpha_num_l + 0.01

plt.plot(alpha_value_l, evaluation_alpha_l, "r-+", linewidth=2)
plt.xlabel("Alpha values")
plt.ylabel("Evaluation")
plt.title("Evaluation metrics of different alpha values for Lasso regression")

print (alpha_value_l, len(alpha_value_l))
print (evaluation_alpha_l, len(evaluation_alpha_l))

"""The best alpha value for the Lasso regression is 0.02

(d) Evaluate the model with the top-performing regularization value for each regularization type on the test split using mean squared error. Report the resulting values.
"""

#for ridge regression
ridge_model_c_test = linear_model.Ridge(alpha= 0.39)
ridge_model_c_test.fit(x_poly_train_c, y_train_c)
y_predicted_ridge_c_test = ridge_model_c_test.predict(x_poly_test_c)
evaluation_ridge_c_test = mean_squared_error(y_test_c, y_predicted_ridge_c_test)
print ("The mean squared error for the Ridge regression is: ", evaluation_ridge_c_test)

#for lasso regression 
lasso_model_c_test = linear_model.Lasso(alpha= 0.02)
lasso_model_c_test.fit(x_poly_train_c, y_train_c)
y_predicted_lasso_c_test = lasso_model_c_test.predict(x_poly_test_c)
evaluation_lasso_c_test = mean_squared_error(y_test_c, y_predicted_lasso_c_test)
print ("The mean squared error for the Lasso regression is: ", evaluation_lasso_c_test)

"""#3.# 
(a) Load a real dataset not covered in class that is designed for the classication
problem; e.g., from sklearn.datasets, Kaggle, your own data, etc. (Code)
"""

from sklearn.datasets import load_wine
wineData = load_wine()
print ("Keys of the wine dataset:\n", wineData.keys())

"""3.(b) Create a 60/20/20 train/val/test split of the dataset. (Code)"""

from sklearn.model_selection import train_test_split
x_trainandval_w, x_test_w, y_trainandval_w, y_test_w = train_test_split(wineData.data, wineData.target, test_size=0.2, random_state=42)
x_train_w, x_val_w, y_train_w, y_val_w = train_test_split (x_trainandval_w, y_trainandval_w, test_size=0.25, random_state=42)
print("number samples in training: ", len(x_train_w))
print("number samples in testing: ", len(x_test_w))
print("number samples in validation: ", len(x_val_w))

"""(c) Examine the impact of regularization for this model. To do so, plot performance curves for the Perceptron model when varying the regularization strength for each of the following types of regularization: ridge (l2 penalty) and lasso (l1 penalty).
Vary the regularization strength to have at least 10 different values and evaluate on the validation split using accuracy. Report the regularization type-value pair that returns the best result. (Code and Write-up)
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.linear_model import Perceptron
#Ridge regression
start_alpha_num = 1
finish_alpha_num = 21
increment_alpha_amt = 1
alpha_value_p_ridge = 0.38
preceptron_alpha_ridge = []
accuracy_alpha_ridge = []

for alpha_num in range (start_alpha_num, finish_alpha_num, increment_alpha_amt):
  classifier_ridge = Perceptron(random_state=0, penalty='l2', alpha= alpha_value_p_ridge)
  classifier_ridge.fit(x_train_w, y_train_w)
  accuracy_ridge=classifier_ridge.score(x_val_w, y_val_w)
  preceptron_alpha_ridge.append(alpha_value_p_ridge)
  alpha_value_p_ridge= alpha_value_p_ridge + 0.002
  accuracy_alpha_ridge.append(accuracy_ridge)

# %matplotlib inline
plt.plot(preceptron_alpha_ridge, accuracy_alpha_ridge, "b-+", linewidth=2)
plt.xlabel("alpha value for Perceptron with Ridge regularization")
plt.ylabel("Accuracy")
plt.title("Impact of regularization with Ridge model")
print (accuracy_alpha_ridge, len (accuracy_alpha_ridge), preceptron_alpha_ridge, len (preceptron_alpha_ridge))

"""The best performance with ridge regression is an acurracy of 0.4722 with the alpha value of 0.39."""

# Commented out IPython magic to ensure Python compatibility.
#Lasso regression
start_alpha_num = 1
finish_alpha_num = 21
increment_alpha_amt = 1
alpha_value_p_lasso = 0.6
preceptron_alpha_lasso = []
accuracy_alpha_lasso = []

for alpha_num in range (start_alpha_num, finish_alpha_num, increment_alpha_amt):
  classifier_lasso = Perceptron(random_state=0, penalty='l1', alpha= alpha_value_p_lasso)
  classifier_lasso.fit(x_train_w, y_train_w)
  accuracy_lasso=classifier_lasso.score(x_val_w, y_val_w)
  preceptron_alpha_lasso.append(alpha_value_p_lasso)
  alpha_value_p_lasso= alpha_value_p_lasso + 0.01
  accuracy_alpha_lasso.append(accuracy_lasso)

# %matplotlib inline
plt.plot(preceptron_alpha_lasso, accuracy_alpha_lasso, "r-+", linewidth=2)
plt.xlabel("alpha value for Perceptron with Lasso regularization")
plt.ylabel("Accuracy")
plt.title("Impact of regularization with Lasso model")
print (accuracy_alpha_lasso, len (accuracy_alpha_lasso), preceptron_alpha_lasso, len (preceptron_alpha_lasso))

"""best accuracy is 0.75 with the alpha 0.77

(d) Next, plot the performance of the model on the test set based on the number of epochs used to train the algorithm. Use the single model from the previous step that returns the best performance across both regularization types. Plot a curve which shows the resulting model's performance for at least 10 epoch values. Use accuracy as your evaluation metric. (Code and Write-up)
"""

# Commented out IPython magic to ensure Python compatibility.
start_epoch_num = 1
finish_epoch_num = 21
increment_epoch_amt = 1

epoch_count = 1
pred_scores = []
num_epochs = []

for epoch_num in range (start_epoch_num, finish_epoch_num, increment_epoch_amt):
  classifier_epoch = Perceptron(random_state=42, penalty='l1', alpha= 0.77, max_iter = epoch_count)
  classifier_epoch.fit(x_train_w,y_train_w)
  accuracy_epoch = classifier_epoch.score(x_test_w, y_test_w)
  pred_scores.append(accuracy_epoch)
  num_epochs.append(epoch_count)
  epoch_count = epoch_count + 5

# %matplotlib inline
plt.plot(num_epochs, pred_scores, "g-+", linewidth=2)
plt.xlabel("Epoch counts")
plt.ylabel("Accuracy")
plt.title("Impact of epochs on the accuracy with lasso regularization")
print (num_epochs, len (num_epochs), pred_scores, len (pred_scores))